{ 
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This is part four in a series of blogs posts on working with data from the USDA NASS database. It follows the third article <a href=\"\" class=\"inlinelink\"> insert info here</a>. In this post, we'll be using <a href=\"\" class=\"inlinelink\">JupyterLab</a> to request data from the NASS database using their API. Since we've already established our <a href=\"\" class=\"inlinelink\">credentials</a>, set up our infrastructure, and deployed our working container, we're ready to retrieve data and send it to AWS for storage. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Before we can start requesting data, we need to import a number of modules that will help us make proper requests and format the returned information. Explaining each module is beyond the scope of this article but I'll point out that _boto3_ and _requests_ are two of the most important. _Requests_ helps us create http requests using Python and _boto3_ is AWS' Python software development kit. At the bottom of the next cell are environment variables imported from the new container. These variables will allow us to query the NASS database and upload our data to S3 at the end."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [],
      "source": [
        "import ndjson\n",
        "import requests\n",
        "import random\n",
        "import os\n",
        "import time\n",
        "import copy\n",
        "import sys\n",
        "from tqdm import tqdm_notebook as tn\n",
        "import boto3.session\n",
        "key = os.environ.get(\"USDAKEY\")\n",
        "bucket = os.environ.get(\"CROP_BUCKET\")\n",
        "profile = os.environ.get(\"AWS_LP\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now that we have the modules and variables set, we can make a test GET request to the NASS database. I'm going to retrieve information about North Carolina. Specifically, I'll select records based on a few headers/columns including <em>commodity_desc</em>, <em>group_desc</em>, <em>statisticcat_desc</em>, <em>unit_desc</em>, and <em>state_alpha</em>. Columns can be found <a href=\"https://quickstats.nass.usda.gov/api\" class=\"inlinelink\">here</a>. Be sure to pass in your API **key** by formatting the request string. Place a lone __*f*__ at the beginning; it being the only character outside the string."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "nc = (\n",
        "    requests.get(\n",
        "        f'''http://quickstats.nass.usda.gov/api/api_GET/?key={key}&\n",
        "        group_desc=INCOME&commodity_desc=COMMODITY+TOTALS&\n",
        "        statisticcat_desc=SALES&unit_desc=$&state_alpha=NC&format=json'''\n",
        "    ).json()['data']\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This request returns a list of 9627 records."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "9627"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(nc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "By selecting the first record in the list, we can examine a sample of the data we received."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'county_name': '',\n",
              " 'util_practice_desc': 'ALL UTILIZATION PRACTICES',\n",
              " 'domain_desc': 'OPERATORS',\n",
              " 'end_code': '00',\n",
              " 'state_ansi': '37',\n",
              " 'Value': '6,612,983,000',\n",
              " 'source_desc': 'CENSUS',\n",
              " 'country_name': 'UNITED STATES',\n",
              " 'state_alpha': 'NC',\n",
              " 'group_desc': 'INCOME',\n",
              " 'county_ansi': '',\n",
              " 'class_desc': 'ALL CLASSES',\n",
              " 'statisticcat_desc': 'SALES',\n",
              " 'watershed_code': '00000000',\n",
              " 'state_name': 'NORTH CAROLINA',\n",
              " 'asd_desc': '',\n",
              " 'region_desc': '',\n",
              " 'reference_period_desc': 'YEAR',\n",
              " 'week_ending': '',\n",
              " 'county_code': '',\n",
              " 'CV (%)': '4.3',\n",
              " 'commodity_desc': 'COMMODITY TOTALS',\n",
              " 'prodn_practice_desc': 'ALL PRODUCTION PRACTICES',\n",
              " 'year': 2012,\n",
              " 'load_time': '2012-12-31 00:00:00',\n",
              " 'short_desc': 'COMMODITY TOTALS - SALES, MEASURED IN $',\n",
              " 'asd_code': '',\n",
              " 'congr_district_code': '',\n",
              " 'sector_desc': 'DEMOGRAPHICS',\n",
              " 'agg_level_desc': 'STATE',\n",
              " 'location_desc': 'NORTH CAROLINA',\n",
              " 'domaincat_desc': 'OPERATORS: (1 OPERATORS)',\n",
              " 'begin_code': '00',\n",
              " 'country_code': '9000',\n",
              " 'unit_desc': '$',\n",
              " 'state_fips_code': '37',\n",
              " 'freq_desc': 'ANNUAL',\n",
              " 'watershed_desc': '',\n",
              " 'zip_5': ''}"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nc[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As you can see, there is quite a bit of data in each record; including the fields we used in our selection critera. Let's grab records for all the states using the same critera. We'll first create a list of all state abbreviations in the <a href=\"https://en.wikipedia.org/wiki/American_National_Standards_Institute\" class=\"inlinelink\">ANSI</a> format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "states = [\n",
        "    'AL', 'AK', 'AZ', 'AR', 'CA', 'CO',\n",
        "    'CT', 'DE', 'FL', 'GA', 'HI', 'ID',\n",
        "    'IL', 'IN', 'IA', 'KS', 'KY', 'LA', \n",
        "    'ME', 'MD', 'MA', 'MI', 'MN', 'MS', \n",
        "    'MO', 'MT', 'NV', 'NE', 'NH', 'NJ', \n",
        "    'NM', 'NY', 'NC', 'ND', 'OH', 'OK',\n",
        "    'OR', 'PA', 'RI', 'SC', 'SD', 'TN',\n",
        "    'TX', 'UT', 'VT', 'VA', 'WA', 'WV',\n",
        "    'WI', 'WY'\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The next step is to define a function that will be called on to do the heavy lifting. We'll create a function called _state_info_ that will send a request containing the abbreviations of each state. It will randomly send each request 1 to 3 seconds apart to space out our requests and not overload the server. As the data is retrieved, the function will perform a minor edit on one of the keys returned. The keys will eventually map to column headers in AWS Athena. The key **\"CV (%)\"** is improperly formatted and Athena will throw an error, accordingly, when attempting to query an improperly formatted column header. We'll then write each data set to a file, name it after the corresponding state information it holds, and save it in our data folder. The files will be saved in <a href=\"http://ndjson.org/\" class=\"inlinelink\">new-line-delimited JSON</a> format. It's great for AWS glue crawlers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "def state_info(state):\n",
        "    \n",
        "    fnames = {}\n",
        "    \n",
        "    for st in tn(state, desc='GET Requests'):\n",
        "        s = (\n",
        "            requests.get(\n",
        "            f'''http://quickstats.nass.usda.gov/api/api_GET/?key={key}&\n",
        "            group_desc=INCOME&commodity_desc=COMMODITY+TOTALS&\n",
        "            statisticcat_desc=SALES&unit_desc=$&state_alpha={st}&format=json''')\n",
        "            .json()['data']\n",
        "        )\n",
        "        for i in s:\n",
        "            i['cv_per'] = i.pop(\"CV (%)\")\n",
        "        \n",
        "        filename = f'state_{st}.ndjson'\n",
        "        fnames[st] = (filename)\n",
        "        with open(f'../data/{filename}', 'w') as filehandle:\n",
        "            ndjson.dump(s, filehandle)\n",
        "        \n",
        "        time.sleep(random.randint(1, 3))\n",
        "    \n",
        "    return(fnames)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "beb027952c064ce3b5fa7bfe2f6afd88",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='GET Requests', max=50, style=ProgressStyle(description_width=…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "si = state_info(states)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Note:** Because we're randomizing the time between each request, the length of time it will take to retrieve all state records can vary."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "After calling the <em>state_info</em> function, the <em>si</em> variable now maps to the <em>fnames</em> dictionary that holds keys consisting of state abbreviations ('NC', 'TZ', etc...) and values consisting of the names of their corresponding ndjson files (state_NC.ndjson)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, pieceing it all together, we can upload our files to AWS S3. We'll call the s3 resource object using the boto3 module and then stream all the data into a bucket and partition the information by state abbreviation. Partitioning will cut down the amount of data scanned at query time saving both, time and money. When we make a request to put the data into our bucket we receive a JSON object response. A 200 status code response means our request was accepted and is 'OK'. Sense we're making a request for each state, we'll send and receive a total of 50 requests and responses. Below, the _upload\\_data_ function will do the work of sending requests and handling the responses. If all goes well, we'll receive a single 200 status code from the function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "s3 = (\n",
        "    boto3.session.Session(profile_name=profile)\n",
        "    .resource('s3')\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [],
      "source": [
        "def upload_data(records):\n",
        "    \n",
        "    all_rs = []\n",
        "    for k, v in tn(records.items(), desc='PUT Requests'):\n",
        "        file = v\n",
        "        with open(f'../data/{file}', 'r') as f:\n",
        "            byt = bytes(ndjson.dumps(ndjson.loads(f.read())).encode('utf-8'))\n",
        "        obj = s3.Object(bucket, f'crop-data/partition_id={k}/{file}')\n",
        "        resp = obj.put(Body=byt)\n",
        "        all_rs.append(resp['ResponseMetadata']['HTTPStatusCode'])\n",
        "        time.sleep(2)\n",
        "    return(set(all_rs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "20f6790d16814b86b1f70767294d21cc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='PUT Requests', max=50, style=ProgressStyle(description_width=…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{200}"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "upload_data(si)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "After receiving a 200 response, we can check our S3 bucket to ensure all data and partitions are present. \n",
        "\n",
        "Well, after all that, we are FINALLY ready to do some data analysis. It's not easy wrangling data, but it's rewarding when it's finished because you learn a lot in the prcoess. You also learn useful coding skills that can be applied when doing an analysis. Think about it, to get to this point we had to learn about AWS resources including: Cloudformation, SAM, S3 and AWS Glue. We're running the current notebook in a docker container and had to use the command line to make it happen. In this notebook, we're using python to wrangle data from a third party API in the USDA. That's a lot of work just to get the data into a format you can query. Exciting stuff. The final blog post will entail exploring the data we've wrangled here."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
	"pelican": {
    "date": "2021-04-24",
    "title": "Fun with USDA NASS Data - Part 4: Querying the NSAA API",
    "slug": "Fun-with-USDA-Data-part4-API",
    "url": "programming/Fun-with-USDA-Data-part4-API",
    "save_as": "programming/Fun-with-USDA-Data-part4-API.html",
	"summary":"<p>this is a test</p>"
  }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
