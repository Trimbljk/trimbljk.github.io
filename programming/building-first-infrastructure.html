<!DOCTYPE html>
<html lang="en">
<head>
		<meta name="viewport" content="width=device-width, initial-scale=1.0">
	<link rel="stylesheet" type="text/css" href="https://trimbljk.github.io/theme/css/custom.css">
	<link rel="stylesheet" type="text/css" href="https://trimbljk.github.io/theme/css/articles.css">
	<link rel="shortcut icon" type="image/svg" href="https://trimbljk.github.io/theme/images/libertyflame.svg">
	<title>building-first-infrastructure</title>

</head>
<body>
	<header class="header">			
		<div class="title-fav">
			<h1 id=top-title>Just Another Data Science Blog...</h1>
			<ul id="social-favi">
				<li class="favi-list">
					<a href="https://www.github.com/Trimbljk"><img class="faviconimg" src="https://trimbljk.github.io/theme/images/github-logo.svg" height="45" width="45"></a></li>
				<li class="favi-list">
			<a href="https://www.linkedin.com/in/jake-trimble/"><img class="faviconimg" src="https://trimbljk.github.io/theme/images/linkedin.svg" height="45" width="45"></a></li>
				<li class="favi-list">
			<a href="https://www.twitter.com/JakeTrimble11/"><img class="faviconimg" src="https://trimbljk.github.io/theme/images/twitter.svg" height="45" width="45"></a></li>
			</ul>
		</div> 
		<nav>
			<a href="https://trimbljk.github.io" class="nav-links">Home</a>
			<a href="https://trimbljk.github.io" class="nav-links">About</a>
			<a href="https://trimbljk.github.io/data-science.html" class="nav-links">Data Science</a>
			<a href="https://trimbljk.github.io/programming.html" class="nav-links">Programming</a>
			<a href="https://trimbljk.github.io/musings.html" class="nav-links">Musings</a>
			<a href="https://trimbljk.github.io/archives.html" class="nav-links">Archives</a>
            	</nav>
        </header>
<main class="main-article-content">
	<div class="article-title">
		Building Infrastructure to Support USDA NASS Data
		<div class="article-date">
			<time class="published" datetime="2020-11-23T00:00:00-05:00">
				2020-November-23
			</time>
		</div>
	</div>
	<div class="article-content">
	<p>Before we start gathering crop data to analyze, we should build some serverless infrastructure that can handle large datasets. Fortunately, most major tech companies (Google, Amazon, Microsoft) have wonderful cloud-platforms that allow customers to build any type of infrastructure to support any application.</p>
<p>We're going to use Amazon Web Service (AWS) to build the infrasture we need to manage the data we collect. If you haven't created AWS credentials yet, go ahead and follow the intructions <a href="https://portal.aws.amazon.com/billing/signup#/start" class="inlinelink">here</a>. Once you're set up, navigate to the <a href="https://aws.amazon.com" class="inlinelink">AWS Homepage</a> and hover over the <em>Products</em> tab, you'll see a variety of solutions from which to choose. For this project, we'll be using <a href="https://aws.amazon.com/s3" class="inlinelink">AWS Simple Storage Service</a> or S3, <a href="https://aws.amazon.com/glue/?nc2=h_ql_prod_an_glu&whats-new-cards.sort-by=item.additionalFields.postDateTime&whats-new-cards.sort-order=desc" class="inlinelink">AWS Glue</a>, and <a href="https://aws.amazon.com/athena/?nc2=h_ql_prod_an_ath&whats-new-cards.sort-by=item.additionalFields.postDateTime&whats-new-cards.sort-order=desc" class="inlinelink">AWS Athena</a>. We'll be using S3 to store the files we request from the USDA NASS API. We'll use Glue to crawl our S3 bucket and then use Athena to query that information. To set it all up, we'll use <a href="https://docs.aws.amazon.com/serverless-application-model/?id=docs_gateway" class="inlinelink">AWS Serverless Application Management (SAM)</a> and <a href="https://aws.amazon.com/cloudformation/?nc2=h_ql_prod_mg_cfA" class="inlinelink">AWS Cloudformation</a>.
Let's get started.</p>
<p>To build our infrastructure, we'll be using a Cloudformation template. This template can be constructed using JSON or YAML format. The template specifies infrastructure needs using code allowing portability. We'll specify our format in YAML (I find it easier to read). Because we're using AWS Serverless Application Management (SAM), instead of just cloudformation, the template will require the following two headers:
<pre class="setpre">
<code class="aws-infrastructure-code"><span class="infra-variable">AWSTemplateFormatVersion</span><span class="colon">:</span><span class="infra-string-value"> '2010-09-09'</span>
<span class="infra-variable">Transform</span><span class="colon">:</span><span class="infra-noq-string-value"> AWS::Serverless-2016-10-31</span></code>
</pre>
Next, we'll specify some resources. We need an S3 bucket to which we'll eventually upload our data, a crawler to pull information from that S3 bucket and a database to place the data once it's been crawled. All resources are grouped under the <strong>Resources</strong> header. First we'll draft our bucket:
<pre class="setpre">
<code class="aws-infrastructure-code"><span class="infra-variable">Resources</span><span class="colon">:</span>
  <span class="infra-variable">OutputBucket</span><span class="colon">:</span>
    <span class="infra-variable">Type</span><span class="colon">:</span><span class="infra-noq-string-value"> AWS::S3::Bucket</span>
    <span class="infra-variable">Properties</span><span class="colon">:</span>
      <span class="infra-variable">BucketName</span><span class="colon">:</span><span class="infra-string-value"> 'jkt-usda-api-crop-data'</span>
      <span class="infra-variable">PublicAccessBlockConfiguration</span><span class="colon">:</span>
        <span class="infra-variable">BlockPublicAcls</span><span class="colon">:</span><span class="infra-noq-string-value"> True</span>
        <span class="infra-variable">BlockPublicPolicy</span><span class="colon">:</span><span class="infra-noq-string-value"> True</span>
        <span class="infra-variable">IgnorePublicAcls</span><span class="colon">:</span><span class="infra-noq-string-value"> True</span>
        <span class="infra-variable">RestrictPublicBuckets</span><span class="colon">:</span><span class="infra-noq-string-value"> True</span></code>
</pre>
We first assign a name to the bucket resource with <strong>OutputBucket</strong> and tell AWS what kind of resource it is using AWS's inherent data convention: <strong>AWS::S3::Bucket</strong>. We then describe what features the bucket maintains under the <strong>Properties</strong> variable. The bucket name must be universal in the S3 namespace. I have also set all my access restrictions to <strong>True</strong> because I want my bucket to be private. Even though private is the default setting on S3, it's good practice to specify it explictly by blocking any public access.</p>
<p>Next, we'll specify our database resource. When the data in our bucket is crawled via AWS Glue, the information will be nicely formatted in the database setforth here. 
<pre class="setpre">
<code class="aws-infrastructure-code">  <span class="infra-variable">Cropdatabase</span><span class="colon">:</span>
    <span class="infra-variable">Type</span><span class="colon">:</span><span class="infra-noq-string-value"> AWS::Glue::Database</span>
    <span class="infra-variable">Properties</span><span class="colon">:</span>
      <span class="infra-variable">CatalogId</span><span class="colon">:</span><span class="aws-intrinsic-func"> !Ref</span><span class="infra-noq-string-value"> AWS::AccountId </span>
      <span class="infra-variable">DatabaseInput</span><span class="colon">:</span>
        <span class="infra-variable">Name</span><span class="colon">:</span><span class="infra-string-value"> "crop_data"</span></code>
</pre>
The database resource is specified by the inherent AWS data convention <strong>AWS::Glue::Database</strong>. It's properties are simple. We give it a name and assign our AWS account ID, using an <a href="https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/intrinsic-function-reference-ref.html" class="inlinelink">instrinsic fuction</a> to the CatalogId object. Each AWS account receives a catalog to which all data can be written and viewed. This isn't the same as S3, even though you store data in S3. The data catalog represents all data across all databases in a unique usable format.</p>
<p>The last resource than needs to be constructed is the crawler. The crawler will read all the files in a specific path in a bucket and construct, or write, the information contained therein to tables that will appear in our database above.
<pre class="setpre">
<code class="aws-infrastructure-code">  <span class="infra-variable">Crawler</span><span class="colon">:</span>
    <span class="infra-variable">Type</span><span class="colon">:</span><span class="infra-noq-string-value"> AWS::Glue::Crawler</span>
    <span class="infra-variable">Properties</span><span class="colon">:</span>
      <span class="infra-variable">Name</span><span class="colon">:</span><span class="infra-string-value"> 'crop-data-crawler' </span>
      <span class="infra-variable">SchemaChangePolicy</span><span class="colon">:</span>
        <span class="infra-variable">UpdateBehavior</span><span class="colon">:</span><span class="infra-noq-string-value"> UPDATE_IN_DATABASE</span>
        <span class="infra-variable">DeleteBehavior</span><span class="colon">:</span><span class="infra-noq-string-value"> DELETE_FROM_DATABASE</span>
      <span class="infra-variable">TablePrefix</span><span class="colon">:</span><span class="infra-noq-string-value"> usda_</span>
      <span class="infra-variable">DatabaseName</span><span class="colon">:</span><span class="aws-intrinsic-func"> !Ref</span><span class="infra-noq-string-value"> Cropdatabase</span>
      <span class="infra-variable">Targets</span><span class="colon">:</span>
        <span class="infra-variable">S3Targets</span><span class="colon">:</span>
          <span class="infra-list-dash">-</span><span class="infra-variable"> Path</span><span class="colon">:</span><span class="aws-intrinsic-func"> !Sub</span>
            <span class="infra-list-dash">-</span><span class="infra-string-value"> 's3://${Bucket}/crop-data'</span>
            <span class="infra-list-dash">-</span><span class="colon"> { </span><span class="infra-variable">Bucket</span><span class="colon">:</span><span class="infra-noq-string-value"> !Ref OutputBucket</span><span class="colon"> }</span>
      <span class="infra-variable">Schedule</span><span class="colon">:</span>
        <span class="infra-variable">ScheduleExpression</span><span class="colon">:</span><span class="infra-string-value"> 'cron(0 0 ? * MON *)'</span>
      <span class="infra-variable">Role</span><span class="colon">:</span><span class="infra-noq-string-value"> AWSglueServiceRole</span></code>
</pre>
So, there's a lot going on here in this code. Again we give the resource a name in the template, <strong>Crawler</strong> and assign it it's AWS resource name <strong>AWS::Glue::Crawler</strong>. We then provide it name as will appear on AWS, which is <strong>crop-data-crawler</strong>. Like the others, the names have conventions in which they have to be lowercase. Schema change policy is dictating the proper operations the crawler will conduct we data is deleted or added to it's target path. For example, if you originally had 5 columns located at your target path and updated or deleted a column where the path now contained 6 or 4 respectively, the crawler knows to add in the new column or delete the old data from the data catalog. The table prefix is what's assigned to the data catalog, after crawling, and appears when using AWS Athena. The target database can be referenced using the intrinsic function, <strong>! Ref</strong>. Next, the targets are specified. A path is declared inside the bucket, and since we want to reference our bucket we built earlier, we can, again, use an intrinsic function <strong>! Ref</strong>. A schedule is set, though it's not necessary if you're not constantly updating your schema. Finally, we specify a Role. This is very important. Without a Role, the crawler will not have the security credentials it needs to access the data in the bucket, or any other resrouce. I'm not going to go into detail, because it's not in the scope of this post, but you can view more about Roles, <a href="https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles.html" class="inlinelink">here</a>.</p>
<p>Once we have our template prepared, we can use the AWS SAM CLI to deploy our infrastructure.
<pre class="setpre">
<code class="aws-infrastructure-code"><span class="infra-variable">AWSTemplateFormatVersion</span><span class="colon">:</span><span class="infra-string-value"> '2010-09-09'</span>
<span class="infra-variable">Transform</span><span class="colon">:</span><span class="infra-noq-string-value"> AWS::Serverless-2016-10-31</span></code>
</pre>
INSERT PICTURE OF THE SAM DEPLOY COMMANDS</p>
<p>You should receive a success notification at the CLI. If not you'll have to determine what the are was and fix it accordingly. Once our infrastructure is deployed, we can shift back to thinking about what data we want to collect from the USDA NASS website. </p>
	</div>
</main>
	<footer class="footer">
		<div>Copyright Â© 2021 Jake Trimble</div>
		<div class="pelican-info">
		<address class="address">
			Proudly powered by Pelican
		</address><!-- /#about -->
		<div>Theme by <a href="https://github.com/Trimbljk">@trimbljk</a></div>
		</div>
	</footer>
</body>
</html>